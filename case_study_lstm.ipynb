{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8468a7ef-a2e2-4d0f-aa0b-5f7ffbf31782",
   "metadata": {},
   "source": [
    "# LSTM for Text Classification\n",
    "\n",
    "Di notebook ini, kita akan berkenalan dengan salah satu arsitektur *neural networks* yang sering digunakan untuk data sekuensial, yaitu **Long Short-Term Memory (LSTM)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced645b4-255f-46be-85ec-70b4dd113641",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "Agenda kita hari ini:\n",
    "* Why LSTM\n",
    "* LSTM dengan PyTorch\n",
    "* Bagaimana cara kerja LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afa7e14-08a6-4d37-a57a-6f72873146e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from string import punctuation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a23efe8-3f3b-4cc1-a358-e8564f9004b8",
   "metadata": {},
   "source": [
    "## Why LSTM?\n",
    "\n",
    "Misalkan kita diberikan potongan berita seperti berikut:\n",
    "\n",
    "> Timnas Indonesia U-22 lolos ke babak final SEA Games 2019 dengan perjuangan berat\n",
    "\n",
    "dan juga seperti berikut:\n",
    "\n",
    ">  Pedangdut Selvi Kitty masih mengupayakan yang terbaik bagi kesembuhan putranya, Abizard Kavin Suseno yang mengidap penyakit langka, sindrom Kawasaki.\n",
    "\n",
    "Kira-kira 2 berita tersebut masuk ke kategori apa?\n",
    "\n",
    "---\n",
    "\n",
    "Setiap kata yang menyusun kalimat di atas, saling terhubung dengan kata sebelum ataupun sesudahnya. Bahkan, mungkin bisa terhubung dengan kata-kata jauh sebelum atau juga setelahnya. Inilah yang menjadi tantangan dalam memproses data teks.\n",
    "\n",
    "Model seperti CNN ataupun ANN memiliki keterbatasan jika diharuskan belajar dari data sekuensial seperti data teks. Salah satu keterbatasan yang ada dalam deep neural networks atau bahkan convolutional neural networks adalah kedua jenis arsitektur tersebut menerima input vektor dalam bentuk atau ukuran yang tetap (gambar) dan menghasilkan output vektor dengan ukuran yang tetap juga (probabilitas tiap kelas)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322d09a0-109b-4878-af32-1f64fe7c4e7c",
   "metadata": {},
   "source": [
    "## LSTM with PyTorch\n",
    "\n",
    "Kita akan membuat model LSTM untuk mengklasifikasikan kategori dari artikel berita."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e761f484-1ed2-499d-aa65-e3ec947ffa94",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "Data yang akan digunakan adalah data artikel yang sudah dikumpulkan yang terdiri dari 9 jenis kategori:\n",
    "* football\n",
    "* international news\n",
    "* health\n",
    "* politik\n",
    "* business\n",
    "* celebs\n",
    "* local news\n",
    "* romance\n",
    "* religi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebea593-116a-4284-8f9c-077e0ca71479",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data/news-article\")\n",
    "DATA_FILEPATH = DATA_DIR / \"news.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3ce755-76ed-445f-9b0d-5d164988762d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news = pd.read_csv(DATA_FILEPATH)\n",
    "df_news = df_news[~df_news[\"class\"].isin([\"international_film_tv\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54939c0f-3cc5-42b2-aa15-3a18c05b385f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22535042-3b96-4ed8-a9da-00735eded468",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceadf3e-0db9-4b0f-9cb1-1a828b53d235",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lowerize(df):\n",
    "    df[\"full_text\"] = df[\"full_text\"].str.lower()\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_punctuation(df):\n",
    "    df[\"full_text\"] = df[\"full_text\"].apply(\n",
    "        lambda excerpt: \"\".join([char for char in excerpt if char not in punctuation])\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_digits(df):\n",
    "    df[\"full_text\"] = df[\"full_text\"].apply(\n",
    "        lambda excerpt: re.sub(r\"\\b\\d+\\b\", \"\", excerpt)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "df_news = df_news.pipe(lowerize).pipe(remove_punctuation).pipe(remove_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb97701a-10fd-4879-ba78-f2d12b0290ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4d60cd-c13e-4c6d-a55a-70074a29268e",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7033ca-d8b0-4619-b770-f46fae9b986f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create dict vocab\n",
    "count = Counter(\" \".join(df_news[\"full_text\"].tolist()).split())\n",
    "vocab = sorted(count, key=count.get, reverse=False)\n",
    "vocab_to_int = {word: i for i, word in enumerate(vocab, 1)}\n",
    "int_to_vocab = {i: word for word, i in vocab_to_int.items()}\n",
    "print(\"Number of vocab:\", len(vocab))\n",
    "\n",
    "# tokenize\n",
    "news_tokens = []\n",
    "for news in df_news[\"full_text\"]:\n",
    "    news_tokens.append([vocab_to_int[word] for word in news.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf14e992-1d0a-49aa-9d31-5a72bcbfdaf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822e3a4a-8753-42b7-8237-7d1d0af207d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.loc[1, \"full_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fd7780-0222-4d04-b22d-9fd689ebd87e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(news_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b805e75d-6227-42d4-854c-8423c7237bba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\" \".join([int_to_vocab[token] for token in news_tokens[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83beeb8-1274-4b86-9c91-3b35e7d4ce36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "news_lengths = Counter([len(x) for x in news_tokens])\n",
    "print(\"Maximum news length:\", max(news_lengths))\n",
    "print(\"Minimum news length:\", min(news_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6febc6a6-5b56-472b-b9e1-948a623f9875",
   "metadata": {},
   "source": [
    "#### Padding Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e20566-c90b-4a9c-a969-d1e45a42354d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_sequence(sequences, seq_length):\n",
    "    \"\"\"\n",
    "    Return sequences where each sequence is padded with 0's\n",
    "    or truncated to the `seq_length`\n",
    "    \"\"\"\n",
    "    padded_sequences = np.zeros((len(sequences), seq_length), dtype=int)\n",
    "\n",
    "    for i, row in enumerate(sequences):\n",
    "        padded_sequences[i, -len(row):] = np.array(row, dtype=int)[:seq_length]\n",
    "\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7845302-83fe-405e-a7e0-010e363fc384",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 500\n",
    "\n",
    "padded_sequences = pad_sequence(news_tokens, SEQ_LENGTH)\n",
    "\n",
    "# assert to check all goes as expected\n",
    "assert len(padded_sequences) == len(news_tokens)\n",
    "assert len(padded_sequences[0]) == SEQ_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b62633c-20f7-4ddb-8f29-017cdf6e206a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(padded_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1069da14-8e27-4726-a2c9-38b364179046",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(news_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff09172-c025-4c42-ad8d-2e6fa92279dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.where(padded_sequences == 0, 1, 0)[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991ef679-953a-4ff8-b573-c6e371a4d93e",
   "metadata": {},
   "source": [
    "#### Encode Target Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401591ed-f43a-49be-9de7-338053f30e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_to_idx = {c: idx for idx, c in enumerate(df_news[\"class\"].unique())}\n",
    "idx_to_class = {idx: c for c, idx in class_to_idx.items()}\n",
    "df_news[\"encoded_class\"] = df_news[\"class\"].map(class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9645fe7b-6b49-46f2-b74e-c81e7bce570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d009d9-30aa-493f-a1e9-6f5e06180f53",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b905cc3-e844-4565-8543-892b3a17b423",
   "metadata": {},
   "source": [
    "### Training, Validation, Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58656e2-fd2b-471f-afe2-0df385f1ae0e",
   "metadata": {},
   "source": [
    "#### Shuffle Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e353db6-784f-4a2f-9ef1-52a3a0ed122c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = padded_sequences.copy()\n",
    "labels = df_news[\"encoded_class\"].values.copy()\n",
    "\n",
    "rng = np.random.default_rng(11)\n",
    "rng.shuffle(features)\n",
    "rng.shuffle(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b62716-c107-45fd-91c9-8155430bb3d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_SIZE = .8\n",
    "\n",
    "train_idx = int(len(features) * TRAIN_SIZE)\n",
    "X_train, X_remaining = features[:train_idx], features[train_idx:]\n",
    "y_train, y_remaining = labels[:train_idx], labels[train_idx:]\n",
    "\n",
    "test_idx = int(len(X_remaining) * .5)\n",
    "X_val, X_test = X_remaining[:test_idx], X_remaining[test_idx:]\n",
    "y_val, y_test = y_remaining[:test_idx], y_remaining[test_idx:]\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{} {}\".format(X_train.shape, y_train.shape), \n",
    "      \"\\nValidation set: \\t{} {}\".format(X_val.shape, y_val.shape),\n",
    "      \"\\nTest set: \\t\\t{} {}\".format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bee148-c784-46e1-b0e9-b2d7555bed3e",
   "metadata": {},
   "source": [
    "#### Data Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fb16b0-0f4c-43c8-8446-b0f49c3fba1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "valid_data = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
    "test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34b8e86-0c6f-4935-bbe8-f4896dba5cbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "X_sample, y_sample = next(dataiter)\n",
    "\n",
    "print('Sample input size: ', X_sample.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', X_sample)\n",
    "print()\n",
    "print('Sample label size: ', y_sample.size()) # batch_size\n",
    "print('Sample label: \\n', y_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1964a21a-ffe1-4f3a-865c-071b02f102a9",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a613b2a0-e211-4786-888c-a41ba65f550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_using_gpu = torch.cuda.is_available()\n",
    "\n",
    "if is_using_gpu:\n",
    "    print(\"Will use GPU for modeling\")\n",
    "else:\n",
    "    print(\"No GPU available, will use CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad1170c-95ff-4ae0-80e1-fef6cde24dcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NewsLSTM(nn.Module):\n",
    "    \"\"\"LSTM model for News Category Classification.\"\"\"\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=.5):\n",
    "        \"\"\"Init model.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim=embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            dropout=drop_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(.3)\n",
    "\n",
    "        # linear and sigmoid layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, hidden: torch.Tensor):\n",
    "        \"Perform forward propagation.\"\n",
    "        batch_size = x.size()\n",
    "\n",
    "        # embedding and LSTM outs\n",
    "        x = x.long()\n",
    "        embed_outs = self.embedding(x)\n",
    "        lstm_outs, hidden = self.lstm(embed_outs, hidden)\n",
    "\n",
    "        # get the last sequence step outputs\n",
    "        lstm_outs = lstm_outs[:, -1, :]\n",
    "\n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_outs)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # sigmoid\n",
    "        # out = self.softmax(out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"Init hidden state of LSTM layer.\"\n",
    "        # Create two new tensors with size: n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (is_using_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82662a90-f76f-4eed-beba-bad92f6e4b55",
   "metadata": {},
   "source": [
    "#### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2877e2-6406-4a32-ab8d-0ad0befa2a2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 4\n",
    "GRAD_CLIP = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652996dd-9d11-4195-a579-00cd6bad8f61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model, train_loader, valid_loader,\n",
    "    criterion, optimizer, print_every=50,\n",
    "    epochs=EPOCHS, grad_clip=GRAD_CLIP, batch_size=BATCH_SIZE\n",
    "):\n",
    "    if is_using_gpu:\n",
    "        model.cuda()\n",
    "\n",
    "    # set mode to training\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = model.init_hidden(batch_size)\n",
    "\n",
    "        for counter, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            if is_using_gpu:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # create new instance for hidden state\n",
    "            # so we don't backprop to the entire training history\n",
    "            h = tuple([weight.data for weight in h])\n",
    "\n",
    "            # zeros gradient\n",
    "            model.zero_grad()\n",
    "\n",
    "            # forward propagation\n",
    "            output, h = model(inputs, h)\n",
    "            # print(output, output.shape)\n",
    "\n",
    "            # calculate loss and do backprop\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # update weights and clip gradient to avoid exploding gradient\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            # validate after `print_every` batch\n",
    "            if counter % print_every == 0:\n",
    "                val_h = model.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "\n",
    "                # set mode to validation\n",
    "                model.eval()\n",
    "                for inputs, labels in valid_loader:\n",
    "                    # create new instance for hidden state\n",
    "                    # so we don't backprop to the entire training history\n",
    "                    val_h = tuple([weight.data for weight in val_h])\n",
    "\n",
    "                    if is_using_gpu:\n",
    "                        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                    output, val_h = model(inputs, val_h)\n",
    "                    val_loss = criterion(output, labels)\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                model.train()\n",
    "                print(\"Epoch: {}/{}...\".format(epoch, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a87881a-c82c-4cf0-92c3-4a0570d8a50d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training components\n",
    "LEARNING_RATE = 1e-2\n",
    "VOCAB_SIZE = len(vocab_to_int) + 1\n",
    "OUTPUT_SIZE = df_news[\"class\"].nunique()\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "\n",
    "# define model\n",
    "model = NewsLSTM(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    output_size=OUTPUT_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    n_layers=NUM_LAYERS\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "model = train_model(model, train_loader, valid_loader, criterion, optimizer, print_every=25, epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da926948-6562-4eca-8dd5-dbf48037f422",
   "metadata": {},
   "source": [
    "#### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7bbc15-0570-46f6-970d-30d693b065cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "num_correct = 0\n",
    "\n",
    "h = model.init_hidden(BATCH_SIZE)\n",
    "\n",
    "# set mode to eval\n",
    "model.eval()\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    h = tuple([weight.data for weight in h])\n",
    "\n",
    "    if is_using_gpu:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "    output, h = model(inputs, h)\n",
    "\n",
    "    test_loss = criterion(output, labels)\n",
    "    test_losses.append(test_loss.item())\n",
    "\n",
    "    _, prediction = output.max(1)\n",
    "\n",
    "    num_correct += (prediction == labels).sum().item()\n",
    "\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct / len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97599ace-7210-47da-a69f-5e49cb041367",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ca8d96-be8d-4ef0-8d5b-894cdd493464",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(inputs: str):\n",
    "    inputs = inputs.lower()\n",
    "    inputs = \" \".join([char for char in inputs if char not in punctuation])\n",
    "    inputs = re.sub(r\"\\b\\d+\\b\", \"\", inputs)\n",
    "    # tokenize\n",
    "    inputs = [vocab_to_int[word] for word in inputs.split()]\n",
    "    # pad sequence\n",
    "    inputs = pad_sequence([inputs], SEQ_LENGTH)\n",
    "    return torch.from_numpy(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b47069-9be8-4ba6-a438-76b971048131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "excerpt = \"\"\"\n",
    "Pelatih Gillingham, Neil Harris, menurunkan Elkan Baggott sejak awal laga.\n",
    "Bek Timnas Indonesia itu berduet dengan Max Ehmer sebagai bek tengah,\n",
    "sementara di sayap ada Cheye Alexander dan Robbie McKenzie.\n",
    "Kemenangan Gillingham hadir dari gol Lewis Walker pada menit 43.\n",
    "Ini merupakan pertandingan ulang melawan Fylde karena pada sebelumnya\n",
    "laga berakhir imbang dengan skor 1-1 pada 5 November lalu.\n",
    "\"\"\"\n",
    "print(excerpt)\n",
    "\n",
    "inputs = preprocess(excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37298c67-1e47-4d1c-bbd8-a7fc8e3f1ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    h = model.init_hidden(1)\n",
    "    out, h = model(inputs, h)\n",
    "    _, prediction = out.max(1)\n",
    "    prediction = idx_to_class[prediction.item()]\n",
    "\n",
    "print(\"Text:\")\n",
    "print(excerpt)\n",
    "print(\"Predicted news category:\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdad96e-66bb-47f8-b381-22c1e2024906",
   "metadata": {
    "tags": []
   },
   "source": [
    "## How LSTM Works?\n",
    "\n",
    "LSTM sengaja didesain untuk mengatasi permasalahan **long-term dependencies**. Komputasi pada layer LSTM lebih kompleks jika dibandingkan dengan model *neural network* berjenis sekuensial lainnya (misal RNN). Ini dikarenakan LSTM mencoba mempertahankan informasi yang diterima jauh sebelum data saat ini diproses.\n",
    "\n",
    "LSTM menghasilkan 2 output:\n",
    "* long-term memory\n",
    "* short-term memory\n",
    "\n",
    "Komputasi pada layer LSTM melibatkan **2 fungsi aktivasi** sesuai dengan fungsi untuk menghasilkan long-term memory dan juga short-term memory.\n",
    "\n",
    "<div align='center'>\n",
    "     <img src=\"https://sylabs.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F39eea92c-e227-4707-b28c-d377461a528e%2Flstm-diagram.png?table=block&id=b5653706-1884-4ec8-9b06-2cea7564e4fa&spaceId=685593da-9b2b-4a94-b296-d52808c79757&width=2000&userId=&cache=v2\" width=80%/>\n",
    "</div>\n",
    "\n",
    "Jika kita bayangkan 2 input dan 2 output pada layer LSTM sebagai **“gerbang”** masuk dan keluar layer, maka layer LSTM memiliki 4 “gerbang” komputasi, yaitu:\n",
    "* learn gate\n",
    "* forget gate\n",
    "* remember gate\n",
    "* use gate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d3529f-fc55-4ed0-9ca0-256d9a4afd88",
   "metadata": {},
   "source": [
    "### Forget Gate\n",
    "\n",
    "Gerbang komputasi ini akan memilih informasi apa yang akan **dibuang** dan informasi apa yang akan **tetap dipertahankan**. Gerbang ini menggunakan input dari input vektor ke-$t$ yang dikombinasikan dengan hidden state ke-$t-1$ ($[X^{<t>},\\, h^{<t-1>}]$) sebagai input dari fungsi aktivasi sigmoid $f_t$ yang berfungsi sebagai forget factor yang outputnya berada pada interval 0 sampai 1. Maka, output dari forget gate adalah $c^{<t-1>} \\cdot f_t$. Berikut adalah persamaan yang digunakan untuk menghitung forget gate.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_t &= \\sigma(W_f[h^{<t-1>},\\: X^{<t>}] + b_f) \\\\\n",
    "\\text{forget_gate} &= c^{<t-1>} \\cdot f_t\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "<div align='center'>\n",
    "    <img src=\"https://sylabs.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F22a48a00-ba3f-4b9f-a696-29194ad11c80%2Fforget-gate.png?table=block&id=39f7afd4-1905-409b-8e69-aa6702f12a46&spaceId=685593da-9b2b-4a94-b296-d52808c79757&width=1860&userId=&cache=v2\" width=70%/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9f47fa-37f1-46b5-98ac-cbdde4c36f19",
   "metadata": {},
   "source": [
    "### Learn Gate\n",
    "\n",
    "Komputasi pada learn gate ini hanya akan memproses input vektor $X^{<t>}$ dan hidden state $h^{<t-1>}$. Intuisi dari gerbang komputasi ini adalah “menggabungkan\" kedua input vektor tersebut kemudian **mengabaikan sebagian informasi “seperlunya”**. Sehingga, terdapat 2 jenis komputasi pada gerbang ini. Pertama dengan kata kunci “mengabaikan”, berarti kita akan menggunakan fungsi aktivasi sigmoid pada kombinasi kedua input vektor $[X^{<t>},\\: h^{<t-1>}]$ sebagai ignoring factor $i_t$. Setelah itu, proses komputasi yang “menggabungkan” kedua informasi tersebut menggunakan fungsi aktivais tanh. Berikut adalah persamaan yang digunakan.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "i_t &= \\sigma(W_i[X^{<t>},\\: h^{<t-1>}] + b_i) \\\\\n",
    "\\text{learn_gate} &= \\tanh(W_n[X^{<t>},\\: h^{<t-1>}] + b_i) \\cdot i_t \\\\\n",
    "&= \\tilde{C}_t \\cdot i_t\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "<div align='center'>\n",
    "    <img src=\"https://sylabs.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F4f159c27-0689-448f-9c29-c7f6b6e941f2%2Flearn-gate.png?table=block&id=998e9a10-7274-4d87-a296-8c941a19cf84&spaceId=685593da-9b2b-4a94-b296-d52808c79757&width=1810&userId=&cache=v2\" width=70%/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98991804-32c1-40b9-aae3-2065d532df01",
   "metadata": {},
   "source": [
    "### Remember Gate\n",
    "\n",
    "Gerbang komputasi ini menggabungkan kedua gerbang sebelumnya, yaitu **forget gate** dan **learn gate**. Ini artinya, remember gate akan menghasilkan output yang akan digunakan sebagai long-term memory atau disebut juga sebagai cell state pada baris ke-$t$, $c^{<t>}$. Proses komputasi pada gerbang ini sangat sederhana karena kita hanya menjumlahkan hasil forget gate dan learn gate sebagai berikut.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "c^{<t>} &= \\text{forget_gate} + \\text{learn_gate} \\\\\n",
    "&= c^{<t-1>} \\cdot f_t + \\tilde{C}_t \\cdot i_t\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://sylabs.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F94884e35-2f15-49c2-8446-87bfcffd0221%2Fremember-gate.png?table=block&id=4e90725f-16bd-45ae-9c1b-f5b220792c67&spaceId=685593da-9b2b-4a94-b296-d52808c79757&width=1790&userId=&cache=v2\" width=70%/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6135c47e-0a29-4e44-82de-cd9632ba1b55",
   "metadata": {},
   "source": [
    "### Use gate\n",
    "\n",
    "Komputasi pada gerbang ini akan menghasilkan output yang akan diteruskan ke layer selanjutnya sekaligus berfungsi sebagai short-term memory baru atau hidden state, $h_t$. Berikut adalah persamaan yang digunakan untuk komputasi pada use gate.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "o_t &= \\sigma(W_o[h^{<t-1>},\\: X^{<t>}] + b_o) \\\\\n",
    "h^{<t>} &= o_t * \\tanh{c^{<t>}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://sylabs.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F7d43ea1e-d77b-49c9-95fa-d04aac804aad%2Fuse-gate.png?table=block&id=ddfd2fda-35af-46ca-a068-bf308421caf2&spaceId=685593da-9b2b-4a94-b296-d52808c79757&width=1780&userId=&cache=v2\" width=70%/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b98618-ef3b-4452-b6a6-6f541986fbd5",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://sylabs.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F4317adf2-9ac7-467d-9c3f-d829ebfdd68b%2Flstm-symbol.png?table=block&id=fc4eea6f-d311-4b74-9d8d-b96b91dd9bb9&spaceId=685593da-9b2b-4a94-b296-d52808c79757&width=1400&userId=&cache=v2\" width=70%/>\n",
    "</div>\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/f6bd87e8-eefc-4805-8117-f933c9acc30c/animated-lstm.gif?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20221116%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221116T040918Z&X-Amz-Expires=86400&X-Amz-Signature=09fa5a1660aa745c5f728cd696568f937c36b73e7885a5bb8a2996226c7317d2&X-Amz-SignedHeaders=host&x-id=GetObject\" width=70%/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
