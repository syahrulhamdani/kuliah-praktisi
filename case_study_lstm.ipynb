{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8468a7ef-a2e2-4d0f-aa0b-5f7ffbf31782",
   "metadata": {},
   "source": [
    "# LSTM for Text Classification\n",
    "\n",
    "Di notebook ini, kita akan berkenalan dengan salah satu arsitektur *neural networks* yang sering digunakan untuk data sekuensial, yaitu **Long Short-Term Memory (LSTM)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced645b4-255f-46be-85ec-70b4dd113641",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "Agenda kita hari ini:\n",
    "* Why LSTM\n",
    "* LSTM dengan PyTorch\n",
    "* Bagaimana cara kerja LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afa7e14-08a6-4d37-a57a-6f72873146e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from string import punctuation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a23efe8-3f3b-4cc1-a358-e8564f9004b8",
   "metadata": {},
   "source": [
    "## Why LSTM?\n",
    "\n",
    "Misalkan kita diberikan potongan berita seperti berikut:\n",
    "\n",
    "> Timnas Indonesia U-22 lolos ke babak final SEA Games 2019 dengan perjuangan berat\n",
    "\n",
    "dan juga seperti berikut:\n",
    "\n",
    ">  Pedangdut Selvi Kitty masih mengupayakan yang terbaik bagi kesembuhan putranya, Abizard Kavin Suseno yang mengidap penyakit langka, sindrom Kawasaki.\n",
    "\n",
    "Kira-kira 2 berita tersebut masuk ke kategori apa?\n",
    "\n",
    "---\n",
    "\n",
    "Setiap kata yang menyusun kalimat di atas, saling terhubung dengan kata sebelum ataupun sesudahnya. Bahkan, mungkin bisa terhubung dengan kata-kata jauh sebelum atau juga setelahnya. Inilah yang menjadi tantangan dalam memproses data teks.\n",
    "\n",
    "Model seperti CNN ataupun ANN memiliki keterbatasan jika diharuskan belajar dari data sekuensial seperti data teks. Salah satu keterbatasan yang ada dalam deep neural networks atau bahkan convolutional neural networks adalah kedua jenis arsitektur tersebut menerima input vektor dalam bentuk atau ukuran yang tetap (gambar) dan menghasilkan output vektor dengan ukuran yang tetap juga (probabilitas tiap kelas)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322d09a0-109b-4878-af32-1f64fe7c4e7c",
   "metadata": {},
   "source": [
    "## LSTM with PyTorch\n",
    "\n",
    "Kita akan membuat model LSTM untuk mengklasifikasikan kategori dari artikel berita."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e761f484-1ed2-499d-aa65-e3ec947ffa94",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "Data yang akan digunakan adalah data artikel yang sudah dikumpulkan yang terdiri dari 9 jenis kategori:\n",
    "* football\n",
    "* international news\n",
    "* health\n",
    "* politik\n",
    "* business\n",
    "* celebs\n",
    "* local news\n",
    "* romance\n",
    "* religi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebea593-116a-4284-8f9c-077e0ca71479",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data/news-article\")\n",
    "DATA_FILEPATH = DATA_DIR / \"news.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3ce755-76ed-445f-9b0d-5d164988762d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news = pd.read_csv(DATA_FILEPATH)\n",
    "df_news = df_news[~df_news[\"class\"].isin([\"international_film_tv\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54939c0f-3cc5-42b2-aa15-3a18c05b385f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22535042-3b96-4ed8-a9da-00735eded468",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceadf3e-0db9-4b0f-9cb1-1a828b53d235",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lowerize(df):\n",
    "    df[\"full_text\"] = df[\"full_text\"].str.lower()\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_punctuation(df):\n",
    "    df[\"full_text\"] = df[\"full_text\"].apply(\n",
    "        lambda excerpt: \"\".join([char for char in excerpt if char not in punctuation])\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_digits(df):\n",
    "    df[\"full_text\"] = df[\"full_text\"].apply(\n",
    "        lambda excerpt: re.sub(r\"\\b\\d+\\b\", \"\", excerpt)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "df_news = df_news.pipe(lowerize).pipe(remove_punctuation).pipe(remove_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb97701a-10fd-4879-ba78-f2d12b0290ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4d60cd-c13e-4c6d-a55a-70074a29268e",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7033ca-d8b0-4619-b770-f46fae9b986f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create dict vocab\n",
    "count = Counter(\" \".join(df_news[\"full_text\"].tolist()).split())\n",
    "vocab = sorted(count, key=count.get, reverse=False)\n",
    "vocab_to_int = {word: i for i, word in enumerate(vocab, 1)}\n",
    "int_to_vocab = {i: word for word, i in vocab_to_int.items()}\n",
    "print(\"Number of vocab:\", len(vocab))\n",
    "\n",
    "# tokenize\n",
    "news_tokens = []\n",
    "for news in df_news[\"full_text\"]:\n",
    "    news_tokens.append([vocab_to_int[word] for word in news.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822e3a4a-8753-42b7-8237-7d1d0af207d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.loc[1, \"full_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fd7780-0222-4d04-b22d-9fd689ebd87e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(news_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b805e75d-6227-42d4-854c-8423c7237bba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\" \".join([int_to_vocab[token] for token in news_tokens[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83beeb8-1274-4b86-9c91-3b35e7d4ce36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "news_lengths = Counter([len(x) for x in news_tokens])\n",
    "print(\"Maximum news length:\", max(news_lengths))\n",
    "print(\"Minimum news length:\", min(news_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6febc6a6-5b56-472b-b9e1-948a623f9875",
   "metadata": {},
   "source": [
    "#### Padding Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e20566-c90b-4a9c-a969-d1e45a42354d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_sequence(sequences, seq_length):\n",
    "    \"\"\"\n",
    "    Return sequences where each sequence is padded with 0's\n",
    "    or truncated to the `seq_length`\n",
    "    \"\"\"\n",
    "    padded_sequences = np.zeros((len(sequences), seq_length), dtype=int)\n",
    "\n",
    "    for i, row in enumerate(sequences):\n",
    "        padded_sequences[i, -len(row):] = np.array(row, dtype=int)[:seq_length]\n",
    "\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7845302-83fe-405e-a7e0-010e363fc384",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 500\n",
    "\n",
    "padded_sequences = pad_sequence(news_tokens, SEQ_LENGTH)\n",
    "\n",
    "# assert to check all goes as expected\n",
    "assert len(padded_sequences) == len(news_tokens)\n",
    "assert len(padded_sequences[0]) == SEQ_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b62633c-20f7-4ddb-8f29-017cdf6e206a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(padded_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1069da14-8e27-4726-a2c9-38b364179046",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(news_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff09172-c025-4c42-ad8d-2e6fa92279dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.where(padded_sequences == 0, 1, 0)[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991ef679-953a-4ff8-b573-c6e371a4d93e",
   "metadata": {},
   "source": [
    "#### Encode Target Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401591ed-f43a-49be-9de7-338053f30e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_to_idx = {c: idx for idx, c in enumerate(df_news[\"class\"].unique())}\n",
    "df_news[\"encoded_class\"] = df_news[\"class\"].map(class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9645fe7b-6b49-46f2-b74e-c81e7bce570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d009d9-30aa-493f-a1e9-6f5e06180f53",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_news.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b905cc3-e844-4565-8543-892b3a17b423",
   "metadata": {},
   "source": [
    "### Training, Validation, Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58656e2-fd2b-471f-afe2-0df385f1ae0e",
   "metadata": {},
   "source": [
    "#### Shuffle Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e353db6-784f-4a2f-9ef1-52a3a0ed122c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = padded_sequences.copy()\n",
    "labels = df_news[\"encoded_class\"].values.copy()\n",
    "\n",
    "rng = np.random.default_rng(11)\n",
    "rng.shuffle(features)\n",
    "rng.shuffle(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b62716-c107-45fd-91c9-8155430bb3d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_SIZE = .8\n",
    "\n",
    "train_idx = int(len(features) * TRAIN_SIZE)\n",
    "X_train, X_remaining = features[:train_idx], features[train_idx:]\n",
    "y_train, y_remaining = labels[:train_idx], labels[train_idx:]\n",
    "\n",
    "test_idx = int(len(X_remaining) * .5)\n",
    "X_val, X_test = X_remaining[:test_idx], X_remaining[test_idx:]\n",
    "y_val, y_test = y_remaining[:test_idx], y_remaining[test_idx:]\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{} {}\".format(X_train.shape, y_train.shape), \n",
    "      \"\\nValidation set: \\t{} {}\".format(X_val.shape, y_val.shape),\n",
    "      \"\\nTest set: \\t\\t{} {}\".format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bee148-c784-46e1-b0e9-b2d7555bed3e",
   "metadata": {},
   "source": [
    "#### Data Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fb16b0-0f4c-43c8-8446-b0f49c3fba1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "valid_data = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
    "test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34b8e86-0c6f-4935-bbe8-f4896dba5cbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "X_sample, y_sample = next(dataiter)\n",
    "\n",
    "print('Sample input size: ', X_sample.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', X_sample)\n",
    "print()\n",
    "print('Sample label size: ', y_sample.size()) # batch_size\n",
    "print('Sample label: \\n', y_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1964a21a-ffe1-4f3a-865c-071b02f102a9",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a613b2a0-e211-4786-888c-a41ba65f550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_using_gpu = torch.cuda.is_available()\n",
    "\n",
    "if is_using_gpu:\n",
    "    print(\"Will use GPU for modeling\")\n",
    "else:\n",
    "    print(\"No GPU available, will use CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad1170c-95ff-4ae0-80e1-fef6cde24dcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NewsLSTM(nn.Module):\n",
    "    \"\"\"LSTM model for News Category Classification.\"\"\"\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=.5):\n",
    "        \"\"\"Init model.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim=embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            dropout=drop_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(.3)\n",
    "\n",
    "        # linear and sigmoid layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, hidden: torch.Tensor):\n",
    "        \"Perform forward propagation.\"\n",
    "        batch_size = x.size()\n",
    "\n",
    "        # embedding and LSTM outs\n",
    "        x = x.long()\n",
    "        embed_outs = self.embedding(x)\n",
    "        lstm_outs, hidden = self.lstm(embed_outs, hidden)\n",
    "\n",
    "        # get the last sequence step outputs\n",
    "        lstm_outs = lstm_outs[:, -1, :]\n",
    "\n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_outs)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # sigmoid\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"Init hidden state of LSTM layer.\"\n",
    "        # Create two new tensors with size: n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (is_using_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215e5272-b9d0-491c-b22a-ef97e1a4d54a",
   "metadata": {},
   "source": [
    "#### Instantiate LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf14e992-1d0a-49aa-9d31-5a72bcbfdaf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(vocab_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82662a90-f76f-4eed-beba-bad92f6e4b55",
   "metadata": {},
   "source": [
    "#### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2877e2-6406-4a32-ab8d-0ad0befa2a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 4\n",
    "GRAD_CLIP = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652996dd-9d11-4195-a579-00cd6bad8f61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model, train_loader, valid_loader,\n",
    "    criterion=criterion, optimizer=optimizer, print_every=50,\n",
    "    epochs=EPOCHS, grad_clip=GRAD_CLIP, batch_size=BATCH_SIZE\n",
    "):\n",
    "    if is_using_gpu:\n",
    "        model.cuda()\n",
    "\n",
    "    # set mode to training\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = model.init_hidden(batch_size)\n",
    "\n",
    "        for counter, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            if is_using_gpu:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # create new instance for hidden state\n",
    "            # so we don't backprop to the entire training history\n",
    "            h = tuple([weight.data for weight in h])\n",
    "\n",
    "            # zeros gradient\n",
    "            model.zero_grad()\n",
    "\n",
    "            # forward propagation\n",
    "            output, h = model(inputs, h)\n",
    "\n",
    "            # calculate loss and do backprop\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "\n",
    "            # update weights and clip gradient to avoid exploding gradient\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            # validate after `print_every` batch\n",
    "            if counter % print_every == 0:\n",
    "                val_h = model.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "\n",
    "                # set mode to validation\n",
    "                model.eval()\n",
    "                for inputs, labels in valid_loader:\n",
    "                    # create new instance for hidden state\n",
    "                    # so we don't backprop to the entire training history\n",
    "                    val_h = tuple([weight.data for weight in val_h])\n",
    "\n",
    "                    if is_using_gpu:\n",
    "                        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                    output, val_h = model(inputs, val_h)\n",
    "                    val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                model.train()\n",
    "                print(\"Epoch: {}/{}...\".format(epoch, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a87881a-c82c-4cf0-92c3-4a0570d8a50d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training components\n",
    "LEARNING_RATE = 1e-2\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "VOCAB_SIZE = len(vocab_to_int) + 1\n",
    "OUTPUT_SIZE = 1\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 2\n",
    "\n",
    "# define model\n",
    "model = NewsLSTM(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    output_size=OUTPUT_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    n_layers=NUM_LAYERS\n",
    ")\n",
    "\n",
    "model = train_model(model, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da926948-6562-4eca-8dd5-dbf48037f422",
   "metadata": {},
   "source": [
    "#### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7bbc15-0570-46f6-970d-30d693b065cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "num_correct = 0\n",
    "\n",
    "h = model.init_hidden(BATCH_SIZE)\n",
    "\n",
    "# set mode to eval\n",
    "model.eval()\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    h = tuple([weight.data for weight in h])\n",
    "\n",
    "    if is_using_gpu:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "    output, h = model(inputs, h)\n",
    "\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "\n",
    "    prediction = torch.round(output.squeeze())\n",
    "\n",
    "    correct_prediction = prediction.eq(labels.float().view_as(prediction))\n",
    "    correct = np.squeeze(correct_prediction.numpy()) if not is_using_gpu else np.squeeze(correct_prediction.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct / len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdad96e-66bb-47f8-b381-22c1e2024906",
   "metadata": {},
   "source": [
    "## How LSTM Works?\n",
    "\n",
    "LSTM sengaja didesain untuk mengatasi permasalahan **long-term dependencies**. Komputasi pada layer LSTM lebih kompleks jika dibandingkan dengan model *neural network* berjenis sekuensial lainnya (misal RNN). Ini dikarenakan LSTM mencoba mempertahankan informasi yang diterima jauh sebelum data saat ini diproses.\n",
    "\n",
    "LSTM menghasilkan 2 output:\n",
    "* long-term memory\n",
    "* short-term memory\n",
    "\n",
    "Komputasi pada layer LSTM melibatkan **2 fungsi aktivasi** sesuai dengan fungsi untuk menghasilkan long-term memory dan juga short-term memory.\n",
    "\n",
    "<div align='center'>\n",
    "     <img src=\"https://sylabs.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F39eea92c-e227-4707-b28c-d377461a528e%2Flstm-diagram.png?table=block&id=b5653706-1884-4ec8-9b06-2cea7564e4fa&spaceId=685593da-9b2b-4a94-b296-d52808c79757&width=2000&userId=&cache=v2\" width=70%/>\n",
    "</div>\n",
    "\n",
    "Jika kita bayangkan 2 input dan 2 output pada layer LSTM sebagai **“gerbang”** masuk dan keluar layer, maka layer LSTM memiliki 4 “gerbang” komputasi, yaitu:\n",
    "* learn gate\n",
    "* forget gate\n",
    "* remember gate\n",
    "* use gate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
