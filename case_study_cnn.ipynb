{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37a6e41b-90a2-4c88-9bab-0482c72f04ea",
   "metadata": {},
   "source": [
    "# CNN for Image Classification\n",
    "\n",
    "Halo, semuanya! üëãüèª\n",
    "\n",
    "Selamat datang di Kuliah Praktisi \"Jaringan Syaraf Tiruan\" atau *Artificial Neural Networks*. Mari kita kenalan dulu!\n",
    "\n",
    "* Saya **Syahrul Bahar Hamdani**, panggil aja **Dani**\n",
    "* Matematika 2012, lulus 2016. Ambil ROK, pakai PSO untuk Penjadwalan Meeting di skripsi, dibimbing oleh Pak Herry dan Bu Auli üôèüèª\n",
    "* Ambil S2 Sains Komputasi ITB tahun 2017 dan lulus 2019. Ambil tesis berjudul \"**_Predictive Maintenance_ Mesin Pesawat dengan Pendekatan _Machine Learning_**\" ([bukti](https://digilib.itb.ac.id/index.php/gdl/view/35771)) yang dibimbing oleh Bu Nuning Nuraini\n",
    "* Sekarang bekerja sebagai **Lead Data Scientist** di [KoinWorks](https://koinworks.com/)\n",
    "\n",
    "Di notebook ini, kita akan menggunakan [PyTorch](https://pytorch.org/https://pytorch.org/) sebagai library utama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668ea923-b74f-43a2-a6e8-97e5697965c8",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "Di notebook ini, kita akan bahas bagaimana membuat model deep learning CNN untuk klasifikasi gambar. Kita akan coba menggunakan _top-down_ approach, dimulai dari hasil akhir model, kemudian kita akan coba bedah komponen penyusunnya.\n",
    "\n",
    "Agenda kita hari ini:\n",
    "* CNN dengan PyTorch\n",
    "* Kenapa menggunakan CNN?\n",
    "* Bagaimana cara kerja CNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8803448-bf1a-419b-9c00-c19661c008c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4d0250-681e-456c-aa4d-376a88526c2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Datasets\n",
    "\n",
    "Kita akan menggunakan data CIFAR-10 yang dikumpulkan oleh Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Dataset CIFAR-10 terdiri dari total 60,000 gambar berukuran **32x32** piksel dalam **10 class**, dengan 6,000 gambar per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7b82ba-4e0a-40f2-90fa-003af0e19af9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc2485b-dceb-4b6e-aadd-9419a6845350",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "transformers = torchvision.transforms.Compose(\n",
    "    [\n",
    "        # transform PIL to tensor\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        # normalize image\n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# training set\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    DATA_DIR,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transformers\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# test set\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    DATA_DIR, train=False, download=True,\n",
    "    transform=transformers\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "class_names = train_dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87908dfb-8adf-4807-8a64-a985db20fbc0",
   "metadata": {},
   "source": [
    "Apa yang terjadi di sini?\n",
    "\n",
    "Kita baru saja mengunduh dataset, jika belum ada, dan memuat data untuk bisa digunakan. Lalu, seperti apa sih dataset nya? Mari kita coba visualisasikan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd44dbe-e39c-4716-a687-8c62f197498c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513d70b4-a1e5-4e0d-b02b-ea5b46e30901",
   "metadata": {},
   "source": [
    "Perhatikan ukuran matriks yang diperoleh dari `train_loader` di bawah ini. Kenapa bisa punya 4 dimensi dengan ukuran `4x3x32x32`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa68bbc-4e37-460a-a1bd-e3b62d588548",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f95b1b-1e5e-45a9-8080-d891fc716e76",
   "metadata": {},
   "source": [
    "Untuk memvisualisasikan gambar, kita gunakan function berikut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0299c7e7-57ac-4678-b515-9cb7c0e1a232",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def imshow(images, labels, grid_size=(2, 2)):\n",
    "    \"Function to visualize image.\"\n",
    "    fig = plt.figure(figsize=(3, 3), tight_layout=True)\n",
    "    for ax in range(1, BATCH_SIZE+1):\n",
    "        np_img = images[ax-1].numpy()*.5 + .5\n",
    "        fig.add_subplot(grid_size[0], grid_size[1], ax)\n",
    "        plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "        plt.title(class_names[labels[ax-1]])\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    plt.show()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e772cf3-9c88-41e3-a1fe-dc324625fce6",
   "metadata": {},
   "source": [
    "Kita bisa jalankan cell di bawah setiap kali kita ingin memvisualisasikan sampel dari dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d2efcf-082d-49c5-aead-cb02523b6e59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for images, labels in train_loader:\n",
    "    break\n",
    "imshow(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07da3942-c83f-4164-b5ec-74bdc682aa59",
   "metadata": {},
   "source": [
    "Jika kita tidak menggunakan PyTorch, kita bisa menggunakan fungsi berikut ini untuk memuat dataset.\n",
    "\n",
    "<details>\n",
    "<summary>Fungsi untuk load data</summary>\n",
    "\n",
    "```python\n",
    "def load_cifar(data_dir, is_train=True):\n",
    "    train_list = [\n",
    "        \"data_batch_1\",\n",
    "        \"data_batch_2\",\n",
    "        \"data_batch_3\",\n",
    "        \"data_batch_4\",\n",
    "        \"data_batch_5\",\n",
    "    ]\n",
    "    test_list = [\n",
    "        \"test_batch\",\n",
    "    ]\n",
    "    meta = {\n",
    "        \"filename\": \"batches.meta\",\n",
    "        \"key\": \"label_names\",\n",
    "    }\n",
    "\n",
    "    if is_train:\n",
    "        downloaded_list = train_list\n",
    "    else:\n",
    "        downloaded_list = test_lit\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # load image data\n",
    "    for file_name in downloaded_list:\n",
    "        file_path = data_dir / file_name\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            entry = pickle.load(f, encoding=\"latin1\")\n",
    "            data.append(entry[\"data\"])\n",
    "            if \"labels\" in entry:\n",
    "                labels.extend(entry[\"labels\"])\n",
    "            else:\n",
    "                labels.extend(entry[\"fine_labels\"])\n",
    "\n",
    "    data = np.vstack(data).reshape(-1, 3, 32, 32)\n",
    "    data = data.transpose(0, 2, 3, 1)\n",
    "\n",
    "    # load metadata\n",
    "    meta_file_path = CIFAR_DIR / meta[\"filename\"]\n",
    "    with open(meta_file_path, \"rb\") as infile:\n",
    "        metadata = pickle.load(infile, encoding=\"latin1\")\n",
    "        classes = metadata[meta[\"key\"]]\n",
    "    class_to_idx = {_class: i for i, _class in enumerate(classes)}\n",
    "\n",
    "    return data, labels, classes, class_to_idx\n",
    "\n",
    "\n",
    "CIFAR_DIR = DATA_DIR / \"cifar-10-batches-py\"\n",
    "train_data, train_labels, train_classes, train_class2idx = load_cifar(CIFAR_DIR, is_train=True)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8269d65e-b82c-4d7e-9609-c126fd23363e",
   "metadata": {},
   "source": [
    "## Representasi Gambar Digital\n",
    "\n",
    "Setiap gambar sebenarnya tersusun atas matriks piksel yang bernilai antara 0 sampai 255. Selain itu, umumnya sebuah gambar terdiri dari 3 unsur warna (*channel*), yaitu merah, hijau, dan biru (RGB). Tapi pada kasus khusus seperti gambar abu-abu (*grayscale*), hanya terdiri dari 1 *channel* saja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d1f770-44cb-45f1-b2fd-b1694f1fce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "unnormalized_img = ((images[0] / 2 + .5)*255).type(torch.uint8)\n",
    "unnormalized_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ec17fe-d4ca-46a0-a814-d1e562468235",
   "metadata": {},
   "source": [
    "Perbedaan akan terlihat jelas untuk gambar dengan 1 channel saja, yaitu *grayscale*, seperti contoh di bawah ini.\n",
    "\n",
    "<div align='center'>\n",
    "<img src=\"https://sylabs.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F6e7f1241-9620-4dab-a730-015c02a24aef%2Fpixel-of-5.png?table=block&id=03028777-803d-4186-9c2c-65bb82aeb26c&spaceId=685593da-9b2b-4a94-b296-d52808c79757&width=1120&userId=&cache=v2\" width=\"40%\"/>\n",
    "</div>\n",
    "\n",
    "Pada contoh gambar di atas, angka 0 merepresentasika warna hitam, 255 merepresentasikan warna putih, dan nilai piksel di antaranya merepresentasikan perubahan warna dari hitam ke putih (abu-abu)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc3be41-9c99-4b26-862e-3870d878a5de",
   "metadata": {},
   "source": [
    "## How Neural Nets Classify Images?\n",
    "\n",
    "Bagaimana kamu bisa tahu bahwa gambar berikut adalah gambar kucing?\n",
    "\n",
    "<div align='center'>\n",
    "    <img src=\"https://sylabs.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fbb386b65-c344-483c-b3cb-9640eea19bc7%2Fcat.jpg?table=block&id=82f2b8e6-ccd4-4df0-a5f8-d078341726be&spaceId=685593da-9b2b-4a94-b296-d52808c79757&width=2000&userId=&cache=v2\" width=50%/>\n",
    "</div>\n",
    "    \n",
    "Sama halnya dengan gambar, komputer ‚Äúmemperhatikan‚Äù **fitur-fitur abstrak** pada suatu objek dari nilai pikselnya. Lalu bagaimana komputer bisa dengan tepat melihat ini? Salah satu caranya adalah dengan menggunakan model deep learning, khususnya convolutional neural network.\n",
    "    \n",
    "Akan tetapi, neural networks dengan *fully-connected layer* akan sangat boros dalam hal jumlah bobot yang akan dilatih. Dengan data CIFAR-10 yang berukuran `32x32x3` piksel, kita akan punyai **3,072** unit pada input layer. \n",
    "\n",
    "<div align='center'>\n",
    "    <img src=\"https://sylabs.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fe903f7be-0e20-44d3-982a-3e189f893bce%2FScreen_Shot_2022-11-13_at_12.35.43.png?table=block&id=0f993f10-3594-490c-b897-2d1f3ebebd6d&spaceId=685593da-9b2b-4a94-b296-d52808c79757&width=2000&userId=&cache=v2\" width=50%/>\n",
    "</div>\n",
    "\n",
    "Oleh karena itu, CNN dibuat untuk mengatasi masalah ini."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb47e40-1ac8-44f8-80f5-df4841ff1979",
   "metadata": {},
   "source": [
    "### CNN\n",
    "\n",
    "Mari kita buat model CNN sederhana menggunakan PyTorch dengan arsitektur:\n",
    "\n",
    "* **2 convolutional layer** yang masing-masing diikuti oleh **Max Pooling layer** dengan masing-masing ukuran kernel secara berturut-turut:\n",
    "    * `6x5x5`\n",
    "    * `16x5x5`\n",
    "* **3 fully-connected layer** dengan jumlah unit pada masing-masing layer secara berturut-turut:\n",
    "    * 120 unit\n",
    "    * 84 unit\n",
    "    * 10 unit (kenapa?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f735f3-9ba2-4a3e-b984-166d6445b3ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60f3d5c-f57d-4227-99bd-b44a734f2444",
   "metadata": {},
   "source": [
    "Apa yang kita lakukan di atas? ü§î"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a341ccf-4073-442d-b10f-ecc8abd206b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conv_layer = nn.Conv2d(3, 6, 5)\n",
    "pool = nn.MaxPool2d(2, 2)\n",
    "print(conv_layer)\n",
    "print(pool)\n",
    "\n",
    "print(\"original shape:\", images[0].shape)\n",
    "print(\"shape after convolution:\", conv_layer(images[0]).shape)\n",
    "print(\"shape after max-pooling:\", pool(conv_layer(images[0])).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b4351d-1a1e-4d05-b283-9139a72d99bd",
   "metadata": {},
   "source": [
    "### Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c712a6e4-8318-454b-a734-7ccb2d1a6abd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, train_loader, valid_loader, num_epochs=2):\n",
    "    valid_loss_min = np.Inf\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):  # loop over the dataset multiple times\n",
    "        train_loss = 0\n",
    "        valid_loss = 0\n",
    "\n",
    "        # train model\n",
    "        model.train()\n",
    "        for batch, data in enumerate(train_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # calculate batch loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # do backprop\n",
    "            loss.backward()\n",
    "\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # update training loss\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # validate model\n",
    "        model.eval()\n",
    "        for batch, data in enumerate(valid_loader, 0):\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "        # calculate average loss\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        valid_loss = valid_loss / len(valid_loader)\n",
    "\n",
    "        print(\n",
    "            'Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "                epoch, train_loss, valid_loss\n",
    "            )\n",
    "        )\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f})...'.format(\n",
    "                valid_loss_min, valid_loss\n",
    "            ))\n",
    "            valid_loss_min = valid_loss\n",
    "\n",
    "    print('Finished Training & Validating')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ae3101-30a9-4c18-a71b-1baac225b5ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(11)\n",
    "\n",
    "# instantiate model\n",
    "model = SimpleCNN()\n",
    "\n",
    "# define loss function\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# define optimization algorithm\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "model = train_model(model, optimizer, loss, train_loader, test_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218c95de-dad9-49e4-904a-9c90d0d60ad7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_params = 0\n",
    "for layer_name, layer_param in model.state_dict().items():\n",
    "    if layer_param.requires_grad:\n",
    "        print(\"Requires grad!\")\n",
    "    print(f\"{layer_name}: {layer_param.numel()}\")\n",
    "    total_params += layer_param.numel()\n",
    "print(\"Total trainable weights:\", total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f63709-11f3-4ec6-9ac2-398ac6443616",
   "metadata": {},
   "source": [
    "### Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499e7fee-d576-4bc6-8438-1e7119368179",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in test_loader:\n",
    "    break\n",
    "\n",
    "imshow(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06a9a06-3b4d-4ab9-8a2b-0f3b8df7b76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(images)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65086018-6100-4a8a-8b7b-b853d327d2a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, predictions = outputs.max(1)\n",
    "\n",
    "imshow(images, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbc7f0e-8981-4dc5-9c03-5bbbe0cd10a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = model(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the model on the test images: {100 * correct // total}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28c7e0d-6e0f-4a3f-bc5c-a6dd284c602d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "# set model mode to evaluation\n",
    "model.eval()\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # calculate the loss\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*inputs.size(0)\n",
    "\n",
    "    # convert output probabilities to predicted class\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(preds.eq(labels.view_as(preds)))\n",
    "\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(BATCH_SIZE):\n",
    "        label = labels.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# calculate and print avg test loss\n",
    "test_loss = test_loss / len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %10s: %2d%% (%2d/%2d)' % (\n",
    "            class_names[i],\n",
    "            100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]),\n",
    "            np.sum(class_total[i])\n",
    "        ))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct),\n",
    "    np.sum(class_total)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703ed3fb-d743-469f-9550-26ca0315701a",
   "metadata": {},
   "source": [
    "## Model Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8df05aa-4685-4604-9368-fe0fe0a46af5",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d68ae8-ec66-46e2-95ce-e6b5016ad303",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = Path(\"models\")\n",
    "\n",
    "if not MODEL_DIR.exists():\n",
    "    MODEL_DIR.mkdir(parents=True, exist_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b58106-48e7-46c0-9d79-e9e5d39716c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model, MODEL_DIR / \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52efa51a-cdc8-44b3-99c8-9693df5717a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), MODEL_DIR / \"model_state.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4476364-2c6c-4f1d-b1f0-37575fa59b77",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef5181a-0cd5-4c6d-9456-cb7ad85c6392",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3bfe51-e778-4e1d-a6d8-61f3252cc3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = torch.load(MODEL_DIR / \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30013488-251b-4588-b1f8-abb8728d73f4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1ddf60-46b2-4500-acd2-52e0d8c571d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = SimpleCNN()\n",
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08afb837-72c0-4754-9572-cc1367a5fb9c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987546d9-1af5-4d24-80e1-d79126110265",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trained_state_dict = torch.load(MODEL_DIR / \"model_state.pth\")\n",
    "cnn.load_state_dict(trained_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebdc4ed-976d-4cf1-8223-140c02264c26",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3ec0cb-6fe2-40f0-92c5-67946934c9f9",
   "metadata": {},
   "source": [
    "### Use Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad8552b-d038-4bd5-b01c-168ea2ff9009",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(DATA_DIR / \"garuda.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c843013-c2b7-424e-9ee1-f5b60ca6b98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de28742-77b9-4866-8ccf-196ee03512e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformers(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514328dd-ae3a-4da3-9087-f7a7438a2ed5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_transformers = torchvision.transforms.Compose(\n",
    "    [\n",
    "        # transform PIL to tensor\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        # normalize image\n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        torchvision.transforms.Resize((32, 32))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd30a7a1-006e-41f6-bd6f-dbb149372717",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformed_img = new_transformers(img)\n",
    "unnormalized_img = ((transformed_img*.5 + .5)*255).type(torch.uint8)\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(np.transpose(unnormalized_img, (1, 2, 0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2807fe9b-bff5-49a1-b841-97a0a6c17ea1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformed_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3dbba1-bdde-4407-9c0d-acb9ef002142",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.unsqueeze(transformed_img, 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8cc50a-9178-47aa-ae68-f45fe222e365",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_model.eval()\n",
    "\n",
    "output = new_model(torch.unsqueeze(transformed_img, 0))\n",
    "_, prediction = output.max(1)\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(np.transpose(unnormalized_img, (1, 2, 0)))\n",
    "plt.title(class_names[prediction])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9a6ae8-7e04-4c03-944c-afd99623502c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## References\n",
    "\n",
    "* [Learning Multiple Layers of Features from Tiny Images](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf), Alex Krizhevsky, 2009.\n",
    "* https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "* https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
